{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.double_conv(in_channels, 64)\n",
    "        self.enc2 = self.double_conv(64, 128)\n",
    "        self.enc3 = self.double_conv(128, 256)\n",
    "        self.enc4 = self.double_conv(256, 512)\n",
    "        self.enc5 = self.double_conv(512, 1024)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.up4 = self.upconv(1024, 512)\n",
    "        self.dec4 = self.double_conv(1024, 512)\n",
    "\n",
    "        self.up3 = self.upconv(512, 256)\n",
    "        self.dec3 = self.double_conv(512, 256)\n",
    "\n",
    "        self.up2 = self.upconv(256, 128)\n",
    "        self.dec2 = self.double_conv(256, 128)\n",
    "\n",
    "        self.up1 = self.upconv(128, 64)\n",
    "        self.dec1 = self.double_conv(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        enc5 = self.enc5(self.pool(enc4))\n",
    "\n",
    "        x = self.up4(enc5)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to extract MSFD.zip and target the MSFD/1/ folder\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_zip\u001b[39m(zip_path, extract_to):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Function to extract MSFD.zip and target the MSFD/1/ folder\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Extract all contents to the specified folder\n",
    "        zip_ref.extractall(extract_to)\n",
    "        print(f\"Extracted ZIP file to {extract_to}\")\n",
    "\n",
    "# Define the UNet Model (same as previously defined)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.enc1 = self.double_conv(in_channels, 64)\n",
    "        self.enc2 = self.double_conv(64, 128)\n",
    "        self.enc3 = self.double_conv(128, 256)\n",
    "        self.enc4 = self.double_conv(256, 512)\n",
    "        self.enc5 = self.double_conv(512, 1024)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.up4 = self.upconv(1024, 512)\n",
    "        self.dec4 = self.double_conv(1024, 512)\n",
    "\n",
    "        self.up3 = self.upconv(512, 256)\n",
    "        self.dec3 = self.double_conv(512, 256)\n",
    "\n",
    "        self.up2 = self.upconv(256, 128)\n",
    "        self.dec2 = self.double_conv(256, 128)\n",
    "\n",
    "        self.up1 = self.upconv(128, 64)\n",
    "        self.dec1 = self.double_conv(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        enc5 = self.enc5(self.pool(enc4))\n",
    "\n",
    "        x = self.up4(enc5)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "\n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# Custom Dataset for loading face images and corresponding masks\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, csv_file, face_crop_dir, face_segmentation_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.face_crop_dir = face_crop_dir\n",
    "        self.face_segmentation_dir = face_segmentation_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Filter out rows where with_mask is False\n",
    "        self.data = self.data[self.data['with_mask'] == 1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Get the image and mask paths\n",
    "        img_name = row['filename']\n",
    "        x1, y1, x2, y2 = row['X1'], row['Y1'], row['X2'], row['Y2']\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.face_crop_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Crop image based on coordinates\n",
    "        img = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.face_segmentation_dir, img_name)\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        mask = mask.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "# Function to prepare transforms for image and mask\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize images to fixed size (256x256)\n",
    "        transforms.ToTensor()  # Convert to tensor\n",
    "    ])\n",
    "\n",
    "# File paths\n",
    "csv_file = 'dataset.csv'  # Path to your dataset CSV file\n",
    "zip_path = 'MSFD.zip'  # Path to the MSFD ZIP file\n",
    "extract_to = 'MSFD_extracted'  # Path where the ZIP will be extracted\n",
    "\n",
    "# Extract MSFD.zip into the specified folder\n",
    "extract_zip(zip_path, extract_to)\n",
    "\n",
    "# Define paths for the extracted folders\n",
    "face_crop_dir = os.path.join(extract_to, 'MSFD/1/face_crop')\n",
    "face_segmentation_dir = os.path.join(extract_to, 'MSFD/1/face_crop_segmentation')\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "transform = get_transforms()\n",
    "dataset = FaceDataset(csv_file, face_crop_dir, face_segmentation_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Training Loop (for UNet)\n",
    "def train_unet(model, dataloader, device):\n",
    "    model.train()\n",
    "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Compute loss (using binary cross entropy)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, masks)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        # Assuming optimizer is defined outside the loop (e.g., Adam)\n",
    "        # optimizer.step()\n",
    "        \n",
    "        print(f\"Batch {batch_idx + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "# Start training\n",
    "train_unet(model, dataloader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
